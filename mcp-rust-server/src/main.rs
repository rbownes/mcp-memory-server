use anyhow::Result;
use rmcp::{
    model::{CallToolResult, Content, Implementation, ProtocolVersion, ServerCapabilities, ServerInfo},
    tool, Error as McpError, ServerHandler, ServiceExt,
    transport::stdio,
};
use std::sync::Arc;
use tracing_subscriber::{self, EnvFilter};

// Module declarations
mod config;
mod embeddings;
mod models;
mod storage;
mod utils;

// Import specific items
use config::Config;
use embeddings::{DummyEmbeddingGenerator, OnnxEmbeddingGenerator, EmbeddingGenerator, EmbeddingError};
use models::{StoreMemoryRequest, RetrieveMemoryRequest, SearchByTagRequest, DeleteMemoryRequest};
use storage::{InMemoryStorage, MemoryStorage};

// Helper functions to convert errors to McpError
fn to_mcp_error(error: anyhow::Error) -> McpError {
    McpError::internal_error(error.to_string(), None)
}

fn embedding_error_to_mcp(error: EmbeddingError) -> McpError {
    McpError::internal_error(error.to_string(), None)
}

#[derive(Clone)]
struct MemoryServer {
    storage: Arc<dyn MemoryStorage>,
}

#[tool(tool_box)]
impl MemoryServer {
    fn new(storage: Arc<dyn MemoryStorage>) -> Self {
        Self { storage }
    }

    #[tool(description = "Store a new memory")]
    async fn store_memory(
        &self,
        #[tool(aggr)] request: StoreMemoryRequest,
    ) -> Result<CallToolResult, McpError> {
        let metadata = request.metadata.unwrap_or_default();
        let content = request.content.clone();
        let content_hash = utils::generate_content_hash(&content, &metadata).map_err(to_mcp_error)?;

        let timestamp = utils::get_current_timestamp();
        let memory = models::Memory {
            content,
            content_hash,
            tags: request.tags.unwrap_or_default(),
            memory_type: request.memory_type,
            timestamp_seconds: timestamp.timestamp(),
            metadata,
            embedding: None, // Will be generated by storage layer
        };

        let (_success, message) = self.storage.store(&memory).await.map_err(to_mcp_error)?;
        Ok(CallToolResult::success(vec![Content::text(message)]))
    }

    #[tool(description = "Retrieve memories semantically similar to the query")]
    async fn retrieve_memory(
        &self,
        #[tool(aggr)] request: RetrieveMemoryRequest,
    ) -> Result<CallToolResult, McpError> {
        // Create a temporary embedding generator for the query
        // In a real implementation, we would use the same embedding generator as the storage
        let embedding_generator = DummyEmbeddingGenerator::new(384);
        let query_embedding = embedding_generator.generate_embedding(&request.query).await
            .map_err(embedding_error_to_mcp)?;

        // Retrieve memories from storage
        let results = self.storage.retrieve(&query_embedding, request.n_results.unwrap_or(5)).await
            .map_err(to_mcp_error)?;

        if results.is_empty() {
            Ok(CallToolResult::success(vec![Content::text(
                "No matching memories found".to_string(),
            )]))
        } else {
            let formatted_results = results
                .iter()
                .enumerate()
                .map(|(i, res)| {
                    format!(
                        "Memory {}:\nContent: {}\nHash: {}\nScore: {:.2}\nTags: {:?}\n---",
                        i + 1,
                        res.memory.content,
                        res.memory.content_hash,
                        res.relevance_score,
                        res.memory.tags
                    )
                })
                .collect::<Vec<_>>()
                .join("\n");

            Ok(CallToolResult::success(vec![Content::text(format!(
                "Found memories:\n{}",
                formatted_results
            ))]))
        }
    }

    #[tool(description = "Search memories by tags")]
    async fn search_by_tag(
        &self,
        #[tool(aggr)] request: SearchByTagRequest,
    ) -> Result<CallToolResult, McpError> {
        let memories = self.storage.search_by_tag(&request.tags).await.map_err(to_mcp_error)?;

        if memories.is_empty() {
            Ok(CallToolResult::success(vec![Content::text(
                "No memories found with the specified tags".to_string(),
            )]))
        } else {
            let formatted_memories = memories
                .iter()
                .enumerate()
                .map(|(i, memory)| {
                    format!(
                        "Memory {}:\nContent: {}\nHash: {}\nTags: {:?}\n---",
                        i + 1,
                        memory.content,
                        memory.content_hash,
                        memory.tags
                    )
                })
                .collect::<Vec<_>>()
                .join("\n");

            Ok(CallToolResult::success(vec![Content::text(format!(
                "Found memories:\n{}",
                formatted_memories
            ))]))
        }
    }

    #[tool(description = "Delete a memory by its hash")]
    async fn delete_memory(
        &self,
        #[tool(aggr)] request: DeleteMemoryRequest,
    ) -> Result<CallToolResult, McpError> {
        let (_success, message) = self.storage.delete(&request.content_hash).await.map_err(to_mcp_error)?;
        Ok(CallToolResult::success(vec![Content::text(message)]))
    }
}

#[tool(tool_box)]
impl ServerHandler for MemoryServer {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            protocol_version: ProtocolVersion::V_2024_11_05,
            capabilities: ServerCapabilities::builder()
                .enable_tools()
                .build(),
            server_info: Implementation {
                name: "mcp-memory-service-rs".to_string(),
                version: "0.1.0".to_string(),
            },
            instructions: Some("This server provides memory storage and retrieval functionality. Use 'store_memory' to store new memories, 'retrieve_memory' for semantic search, 'search_by_tag' to find memories by tags, and 'delete_memory' to remove memories.".to_string()),
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_env_filter(EnvFilter::from_default_env().add_directive(tracing::Level::INFO.into()))
        .with_writer(std::io::stderr)
        .with_ansi(false)
        .init();

    // Load configuration
    let config = Config::load()?;

    // Initialize embedding generator based on configuration
    let embedding_generator: Arc<dyn EmbeddingGenerator> = match config.embedding_model {
        config::EmbeddingModel::Dummy => {
            tracing::info!("Using dummy embedding generator with size {}", config.embedding_size);
            Arc::new(DummyEmbeddingGenerator::new(config.embedding_size))
        },
        config::EmbeddingModel::Onnx => {
            tracing::info!("Using ONNX embedding generator");
            
            // Check if model path is provided
            if let Some(model_path) = config.embedding_model_path.clone() {
                match OnnxEmbeddingGenerator::new(model_path, None, config.embedding_size) {
                    Ok(generator) => {
                        tracing::info!("Successfully initialized ONNX embedding generator");
                        Arc::new(generator)
                    },
                    Err(e) => {
                        tracing::error!("Failed to initialize ONNX embedding generator: {}", e);
                        tracing::info!("Falling back to dummy embedding generator");
                        Arc::new(DummyEmbeddingGenerator::new(config.embedding_size))
                    }
                }
            } else {
                // Try to download a default model
                let model_dir = config.chroma_db_path.join("models");
                let model_name = "all-MiniLM-L6-v2";
                
                match tokio::runtime::Handle::current().block_on(
                    OnnxEmbeddingGenerator::download_if_needed(&model_dir, model_name)
                ) {
                    Ok(model_path) => {
                        match OnnxEmbeddingGenerator::new(model_path, None, config.embedding_size) {
                            Ok(generator) => {
                                tracing::info!("Successfully initialized ONNX embedding generator with downloaded model");
                                Arc::new(generator)
                            },
                            Err(e) => {
                                tracing::error!("Failed to initialize ONNX embedding generator with downloaded model: {}", e);
                                tracing::info!("Falling back to dummy embedding generator");
                                Arc::new(DummyEmbeddingGenerator::new(config.embedding_size))
                            }
                        }
                    },
                    Err(e) => {
                        tracing::error!("Failed to download ONNX model: {}", e);
                        tracing::info!("Falling back to dummy embedding generator");
                        Arc::new(DummyEmbeddingGenerator::new(config.embedding_size))
                    }
                }
            }
        }
    };

    // Initialize storage based on configuration
    let storage: Arc<dyn MemoryStorage> = match config.storage_backend {
        config::StorageBackend::InMemory => {
            tracing::info!("Using in-memory storage");
            Arc::new(InMemoryStorage::new(embedding_generator.clone()))
        },
        config::StorageBackend::ChromaDB => {
            tracing::info!("Using ChromaDB storage");
            if let Some(url) = config.chroma_db_url.clone() {
                // Use remote ChromaDB
                tracing::info!("Connecting to remote ChromaDB at {}", url);
                match storage::ChromaMemoryStorage::new(
                    url,
                    config.chroma_collection_name.clone(),
                    embedding_generator.clone(),
                ).await {
                    Ok(storage) => Arc::new(storage),
                    Err(e) => {
                        tracing::error!("Failed to connect to ChromaDB: {}", e);
                        tracing::info!("Falling back to in-memory storage");
                        Arc::new(InMemoryStorage::new(embedding_generator.clone()))
                    }
                }
            } else {
                // Use local ChromaDB
                tracing::info!("Using local ChromaDB at {:?}", config.chroma_db_path);
                match storage::ChromaMemoryStorage::from_path(
                    config.chroma_db_path.clone(),
                    config.chroma_collection_name.clone(),
                    embedding_generator.clone(),
                ).await {
                    Ok(storage) => Arc::new(storage),
                    Err(e) => {
                        tracing::error!("Failed to initialize local ChromaDB: {}", e);
                        tracing::info!("Falling back to in-memory storage");
                        Arc::new(InMemoryStorage::new(embedding_generator.clone()))
                    }
                }
            }
        }
    };

    // Create and run server
    let service = MemoryServer::new(storage).serve(stdio()).await?;

    tracing::info!("MCP Memory Service running on stdio");

    service.waiting().await?;

    Ok(())
}
